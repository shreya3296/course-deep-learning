{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs109b_hw3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "## Homework 3: Artificial Neural Networks, Model Interpretation, and Regularization\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2024**<br/>\n",
    "**Instructors**: Pavlos Protopapas & Alex Young\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(109)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure notebook runtime\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"background: lightsalmon; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
    "\n",
    "### Instructions\n",
    "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
    "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should includelabels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
    "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "# Notebook Contents\n",
    "\n",
    "- [**PART 1 [50 pts]: Modeling and predictive intervals in ANNs**](#part1)\n",
    "  - [Overview and Data Description](#part1intro)\n",
    "  - [Questions](#part1questions)\n",
    "\n",
    "\n",
    "- [**PART 2 [50 pts]: Kannada MNIST Kaggle competition**](#part2)\n",
    "  - [Problem Statement](#part2intro)\n",
    "  - [The Kannada MNIST Dataset](#part2about)\n",
    "  - [Downloading the Data Files](#part2data)\n",
    "  - [CS109B Kaggle Competition](#part2kaggle)\n",
    "  - [Questions](#part2questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 1 [50 pts]: Model interpretation and predictive intervals in ANNs\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview and Data Description\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this problem, you will be building and interpreting models to predict whether a flight was delayed for its arrival. The model will be based on features that can be measured as the flight takes off.\n",
    "\n",
    "We will also estimate the predictive intervals of the model using bootstrapping. We will utilize those predictive intervals to build a new kind of model: a model that refrains from making a prediction when it is not confident.\n",
    "\n",
    "The variable in the original csv are:\n",
    "\n",
    "    \n",
    "**ARRIVAL_DELAY**: the difference between scheduled arrival and actual arrival, in minutes (positive is late, negative is early).\n",
    "\n",
    "**DISTANCE**: the distance between arrival and departure airports, in miles.\n",
    "\n",
    "**SCHEDULED_TIME**: the flight's scheduled travel time in minutes.\n",
    "\n",
    "**MONTH**: the month the flight took off, 1 = January, 2 = February, etc.\n",
    "\n",
    "**SCHED_DEP_HOUR**: the scheduled departure time (the hour of the day).\n",
    "\n",
    "**SCHED_ARR_HOUR**: the scheduled arrival time (the hour of the day).\n",
    "\n",
    "**FLIGHT_COUNT**: the number of flights flying out of the origin airport before noon on a typical day.\n",
    "\n",
    "**DAY_OF_WEEK**: the day of the week, 1 = Monday, 2 = Tuesday, etc.\n",
    "\n",
    "**ORIGIN_AIRPORT**: the airport the flight took off from.\n",
    "\n",
    "**DESTINATION_AIRPORT**: the airport the flight was scheduled to land at.\n",
    "\n",
    "For the airport codes, see: https://www.bts.gov/topics/airlines-and-airports/world-airport-codes\n",
    "\n",
    "To sucessfully complete this part, you will proceed by fitting a NN model, evaluating its accuracy, interpreting the predictors' importance, and finally evaluating the predictive intervals.\n",
    "\n",
    "**NOTE:** the observations were sampled so that roughly half of the observations were delayed and half of the observations were not delayed.\n",
    "\n",
    "<!-- </div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1solutions\"></a>\n",
    "\n",
    "## PART 1 Questions\n",
    "\n",
    "[Return to contents](#contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1**  **Preprocess the data**\n",
    "\n",
    "**Note that this part (section 1.1) has been completed for you. Simply run the provided cells below to load and preprocess the data. While you do not need to write any code here, you are expected to read through the processing steps taken here so that you fully understand the data you'll be working with.**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1.1**   Read in the dataset `data/flights.csv` into a DataFrame called `df_flights`. Create a new column in the DataFrame called `DELAY_OR_NOT`. This is a binary variable that denotes whether `ARRIVAL_DELAY` is greater-than-or-equal-to 15 minutes (the FAA and BTS define a flight as delayed by 15 minutes or more). This is going to be the response variable for the rest of part 1. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = pd.read_csv(\"data/flights.csv\")\n",
    "df_flights.info()\n",
    "df_flights.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights[\"DELAY_OR_NOT\"] = (df_flights[\"ARRIVAL_DELAY\"] >= 15).astype(int)\n",
    "\n",
    "delay_counts = df_flights[\"DELAY_OR_NOT\"].value_counts().sort_index()\n",
    "\n",
    "print(\n",
    "    \"The resulting counts for our \\\"DELAY_OR_NOT\\\" response variable,\\n\"\n",
    "    \"where class 1 are flights with actual arrivals 15 minutes late\\n\"\n",
    "    \"or later than scheduled, are:\\n\\n\\tclass\\tobservations\"\n",
    ")\n",
    "for resp, value in zip(delay_counts.index, delay_counts.values):\n",
    "    print(\"\\t{}  \\t{:,.0f}\".format(resp, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1.2 Preprocess the data**\n",
    "    \n",
    "- Deal with missing values if there are any\n",
    "- One-hot-encode the non-numeric categorical variables\n",
    "- Split the data using an 80/20 train-test split with `random_state=109` and stratifying on the response variable\n",
    "- Standardize train and test with the scaler fit on the train data\n",
    "\n",
    "Print the resulting shapes of your $X$ and $y$ dataframes for both your train and your test sets.\n",
    "    \n",
    "**NOTE:** While inspecting your data, you may notice that a large number of airport codes are recorded using 5-digit values instead of the expected 3-letter codes. That is perfectly fine. Those 5-digit values should be considered valid and just be treated the same as they would be if they were 3-letter codes.\n",
    "\n",
    "**TIPS:** \n",
    "- month and day-of-the-week should be treated as numerical in this context.\n",
    "- you should consider what predctors are acceptable to include given our goal is to predict if a flight is delayed.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By displaying rows with missing values, we can see\n",
    "# that ALL missing values occur for flights between\n",
    "# BOS and SFO. This would indicate that these values are NOT\n",
    "# missing at random. Therefore, we will want to impute our\n",
    "# values rather than simply delete these rows\n",
    "print(\"ALL MISSING VALUE ROWS, THEY APPEAR TO BE BSO TO SFO FLIGHTS:\")\n",
    "display(df_flights[df_flights.isnull().any(axis=1)])\n",
    "\n",
    "# The next thing we will do is display all BOS to SFO observations\n",
    "# to determine how we should treat missing values in each\n",
    "# predictor column.\n",
    "print(\n",
    "    \"\\nALL BOS TO SFO FLIGHT OBSERVATIONS (INCLD. THOSE WITH MISSINGNESS):\"\n",
    ")\n",
    "loc_filter = (df_flights[\"ORIGIN_AIRPORT\"] == \"BOS\") & (df_flights[\"DESTINATION_AIRPORT\"] == \"SFO\")\n",
    "display(df_flights.loc[loc_filter])\n",
    "\n",
    "# We can see that we know the exact values to enter for missing\n",
    "# DISTANCE and missing FLIGHT_COUNT values, so rather than\n",
    "# impute, we can just explicilty set the values as such, being\n",
    "# certain to fix the column dtypes as well\n",
    "df_flights[\"DISTANCE\"] = df_flights[\"DISTANCE\"].fillna(2704).astype(int)\n",
    "df_flights[\"FLIGHT_COUNT\"] = df_flights[\"FLIGHT_COUNT\"].fillna(172).astype(int)\n",
    "\n",
    "# As for the one missing SCHEDULED_TIME value, we could do\n",
    "# something more involved like creating some sort of imputation model\n",
    "# (such as trying to predict that one value using kNN), but because it's only\n",
    "# one value and there does not appear to be much variability in time among\n",
    "# the flights between BOS and SFO, we can probably just get away with populating\n",
    "# this cell with the mean SCHEDULED_TIME for all flights between BOS and SFO\n",
    "mean_time = df_flights.loc[loc_filter][\"SCHEDULED_TIME\"].mean()\n",
    "df_flights[\"SCHEDULED_TIME\"] = df_flights[\"SCHEDULED_TIME\"].fillna(mean_time).astype(int)\n",
    "\n",
    "# To wrap up, we'll print the df_flights info again to confirm no more\n",
    "# missing values exist\n",
    "print(\"\\nTHE df_flights DATAFRAME AFTER RESOLVING MISSINGNESS:\\n\")\n",
    "display(df_flights.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify our response variable\n",
    "response = \"DELAY_OR_NOT\"\n",
    "\n",
    "# identify categorical predictors for one-hot-encoding\n",
    "cat_preds = [\"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\"]\n",
    "\n",
    "# identify columns we will need to drop from X after encoding \n",
    "drop_cols = [\"ARRIVAL_DELAY\", response] + cat_preds\n",
    "\n",
    "# one-hot-encode and make certain to drop one col for each predictor\n",
    "df_one_hot = pd.get_dummies(df_flights[cat_preds], drop_first=True)\n",
    "\n",
    "# merge encoded columns with other predictor columns to create\n",
    "# our X data, being certain to drop non-predictor columns\n",
    "X_flights = pd.concat(\n",
    "    [\n",
    "        df_flights.drop(columns=drop_cols),\n",
    "        df_one_hot,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "y_flights = df_flights[response]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_flights, y_flights, test_size=0.20, random_state=111, stratify=y_flights\n",
    ")\n",
    "\n",
    "# scale X data using standard scaler, and retain our original\n",
    "# X dataframes so we can use them to plot data in its\n",
    "# original scale for Q1.4 later in HW3\n",
    "X_train_std = X_train.copy()\n",
    "X_test_std = X_test.copy()\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_std[X_train.columns] = scaler.transform(X_train)\n",
    "X_test_std[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "# print our resulting X and y shapes\n",
    "print(\n",
    "    \"The shapes of our resulting X and y train and test sets are:\\n\\n\"\n",
    "    \"\\tX_train\\t{}\\n\\ty_train\\t{}\\n\\n\\tX_test\\t{}\\n\\ty_test\\t{}\\n\"\n",
    "    .format(\n",
    "        X_train.shape, y_train.shape, X_test.shape, y_test.shape,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2**  **Fit an ANN**\n",
    "\n",
    "Fit an artificial neural network model using all predictors (name this model `NN_model`).  Use a dense feed-forward network with two hidden layers with 15 nodes in each hidden layer. For this network, use a reasonable activation functions for the hidden layers and output, select an appropriate loss function and optimizer, specify a validation split of 0.2, and train for a reasonable number of epochs and batch size of your choice. Plot the training accuracy and validation accuracy as a function of epochs from your `NN_model`'s training history. Evaluate the `NN_model` model on both train and test, and print out the resulting train and test accuracies.\n",
    "\n",
    "You should base your choice of \"a reasonable number of epochs\" on a visualization of the model's training history.\n",
    "\n",
    "<a id=\"q12\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build your NN \n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile it and run it\n",
    "# your code here \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot train and val acc as a function of epochs\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate on train and test and print accuracy results\n",
    "# your code here \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3**  **Bootstrap prediction intervals**\n",
    "\n",
    "In HW2, we saw how we could use MCMC to perform Bayesian inference on the parameters of a logistic regression model. This gave us access not only to samples from the posterior distribution (betas) but we could also generate samples from the posterior predictive distribution (data). The posterior predictive can be used to represent our uncertainy about a given prediction, $\\hat{y}$.\n",
    "\n",
    "We'd like to do something similar for our neural network here so that we can quantify the uncertainty in our predictions. But there are some problems. First, we haven't (explicitly) specified any priors on the network weights. Second, and more serious, neural networks have many, many parameters. Far too many for MCMC to be a viable approach. Sampling in such a high dimensional space is simply intractable.\n",
    "\n",
    "Bootstrap to the rescue!\n",
    "\n",
    "Using the same network architecture as `NN_model` (layers, nodes, activations, etc.) and your scaled data from that model, create multiple training sets using bootstrapping and fit a separate neural network model to each bootstrapped set of data (the number of bootstraped *datasets*, $n$, should be at least 50). For each of the $n$ models, make predictions on the test data. Randomly select 8 test observations and on 8 subplots, plot the distribution of the $n$ predicted probabilities with the 95% prediction intervals clearly marked and reported in each subplot and the **actual** class of each observation included in each subplot's title for easy reference.\n",
    "    \n",
    "Interpret what you see in 3-5 sentences.\n",
    "\n",
    "**NOTE:** The code for this problem can take an extremely long time to execute. Please feel free to use the `progressbar` function provided below to visually track the progress of your bootstraps.\n",
    "\n",
    "<a id=\"q13\"></a>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def progressbar(n_step, n_total):\n",
    "    \"\"\"Prints self-updating progress bar to stdout to track for-loop progress\n",
    "    \n",
    "    There are entire 3rd-party libraries dedicated to custom progress-bars.\n",
    "    A simple function like this is often more than enough to get the job done.\n",
    "    \n",
    "    :param n_total: total number of expected for-loop iterations\n",
    "    :type n_total: int\n",
    "    :param n_step: current iteration number, starting at 0\n",
    "    :type n_step: int\n",
    "\n",
    "    .. example::\n",
    "    \n",
    "        for i in range(n_iterations):\n",
    "            progressbar(i, n_iterations)\n",
    "            \n",
    "    .. source:\n",
    "    \n",
    "        This function is a simplified version of code found here:\n",
    "        https://stackoverflow.com/questions/3160699/python-progress-bar/15860757#15860757\n",
    "    \"\"\"\n",
    "    n_step = n_step + 1\n",
    "    barlen = 50\n",
    "    progress = n_step / n_total\n",
    "    block = int(round(barlen * progress))\n",
    "    status = \"\"\n",
    "    if n_step == n_total:\n",
    "        status = \"Done...\\r\\n\\n\"\n",
    "    text = \"\\r [{0}] {1}/{2} {3}\".format(\n",
    "        \"=\" * block + \"-\" * (barlen - block),\n",
    "        n_step,\n",
    "        n_total,\n",
    "        status,\n",
    "    )\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Bootstrap and train your networks and get predictions on X test\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate your plot\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4**  **Build an \"abstain\" bagging model**\n",
    "\n",
    "Using the probability distribution of the predictions obtained from the bootstrapped samples above, we can evaluate how confident we should be in our bagged (i.e. bootstrap-aggregated) predictions for each test observation.\n",
    "\n",
    "To accomplish this, you will first calculate a metric we'll call the **Posterior Prediction Dissent (PPD)** related to the proportion of predictions a given test observation receives for the minority opinion. Some examples: if 10% of the bootstrapped models predict $\\hat{y} = 0$ with the remaining 90% predicting $\\hat{y}=1$, then the $PPD=0.1$. When a bagged prediction's $PPD=0$, all predictions are compatible (i.e. all bootstrapped probabilities for that test observation are on the same side of $\\hat{p}=0.5$). Likewise, when the $PPD=0.5$, half of the bootstrapped predictions for that test observation are $\\hat{y}=0$, and the other half are $\\hat{y}=1$. After calculating your $PPD$ values for all test observations, you should have $n=2000$ $PPD$ values (i.e. one for each test observation).\n",
    "\n",
    "Next, to get more accurate predictions, we can create an **abstain** model that will abstain from making a prediction for a particular observation if some defined threshold for lack-of-confidence (i.e. maximum permissible $PPD$ value) is crossed. (If you'd like to learn more about abstain models, you can read more [here](https://openreview.net/forum?id=rJxF73R9tX).)\n",
    "\n",
    "Let's explore how your resulting test accuracies might change by using your bootstrapped prediction results from question 1.5 for an **abstain bagging model** (i.e. a bootstrap aggregated model where some test observations are simply not predicted based on a given $PPD$ threshold). You can make your abstain model *stricter* by using smaller $PPD$ threshold values.\n",
    "\n",
    "- Print the test accuracy for your **bagging model** predictions from question 1.5 using predictions for all 2,000 of our test observations. \n",
    "\n",
    "- Plot the test accuracies for an **abstain bagging model** using your predictions from question 1.5 as a function of increasing $PPD$.\n",
    "\n",
    "- Also, plot the proportion of test observations not abstained (i.e. the proportion of those predicted) for your **abstain bagging model** as a function of increasing $PPD$.\n",
    "\n",
    "- Interpret what you see in 3-5 sentences.\n",
    "\n",
    "**NOTE**: You should observe that as $PPD$ decreases (more confident predictions), you must also compromise on the number of points that your abstain model is permitted to predict on. \n",
    "\n",
    "**HINT:** \n",
    "- What is the range of values PPD can take on? Is it the same as the range of our predictions themselves?\n",
    "- When calculating accuracies, you should only consider those observations that received predictions (i.e., not the abstained observations)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "\n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 2 [50 pts]: Kannada MNIST Kaggle competition\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "<a id=\"part2intro\"></a>\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "Artificial neural networks (ANNs) can be prone to overfitting, where they learn specific patterns present in the training data, but the patterns do not generalize to new data.\n",
    "\n",
    "There are several methods used to improve ANN generalization. \n",
    "\n",
    "One approach is to use an architecture just barely wide or deep enough to fit the data. The idea here is that smaller networks are less expressive and thus less able to overfit the data.\n",
    "\n",
    "However, it is difficult to know a priori the correct size of the ANN, and it is computationally costly to hunt for the correct size. Given this, other methodologies are used to prevent overfitting and improve ANNs' generalizability. These methodologies, like other techniques that combat overfitting, fall under the umbrella term of \"regularization\".\n",
    "\n",
    "In this problem, you are asked to regularize a network of a given architecture.\n",
    "\n",
    "<a id=\"part2about\"></a>\n",
    "\n",
    "## The Kannada MNIST Dataset\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F1e01bcc28b5ccb7ad38a4ffefb13cde0%2Fwondu.png?generation=1603204077179447&alt=media)\n",
    "\n",
    "\n",
    "For this problem, we will be working with a modified version of the [Kannada MNIST dataset](https://arxiv.org/pdf/1908.01242.pdf), which is a large database of handwritten digits in the indigenous language *Kannada*.\n",
    "\n",
    "This dataset consists of 60,000 28x28 grayscale images of the ten digits, along with a test set of 10,000 images. \n",
    "\n",
    "For this homework, we will simplify the problem by only using the digits labeled `0` and `1` owing to the similarity of the two symbols, and we will use a total of 1,200 samples for training (this includes the data you will use for validation).\n",
    "\n",
    "To understand the dataset better, we recommend this [article](https://towardsdatascience.com/a-new-handwritten-digits-dataset-in-ml-town-kannada-mnist-69df0f2d1456) by Vinay Prabhu, the curator of the dataset.\n",
    "\n",
    "<a id=\"part2data\"></a>\n",
    "\n",
    "## Downloading the Data Files\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "**Please download the specific `kmnist_train.csv` and `kmnist_test.csv` data files available on [the \"Data\" tab of the CS109B HW3 Kaggle Competition website](https://www.kaggle.com/t/01aa5d4183fd4c429e4563808369b2a2). (DO NOT USE DATA FROM ANY OTHER SOURCE!)**\n",
    "\n",
    "Here's a brief description of the data files:\n",
    "\n",
    "- `kmnist_train.csv` is our training dataset and the last column contains our response class. The 784 other columns correspond to the pixel values of the 28x28 dimension image. Class 0 means a sample is the handwritten digit `0` and class 1 means a sample is the handwritten digit `1` in the Kannada written language. `kmnist_train.csv` has 1,200 samples.\n",
    "\n",
    "\n",
    "- `kmnist_test.csv` has a structure similar to `kmnist_train.csv`, however the class label column is NOT included in with the test set. `kmnist_test.csv` has 2,000 samples. \n",
    "\n",
    "\n",
    "Kaggle leaderboard scores are accuracy scores calculated by Kaggle when you upload your predictions on this test set.\n",
    "\n",
    "- `sample_submission.csv` is the format that kaggle will accept. The uploaded `.csv` must contain 2 columns. The first column must be named `id` and needs to contain the test observation index numbers for each prediction, the second must be named `category` and needs to contain your class predictions (i.e. `0` or `1`) for each corresponding test observation index location. \n",
    "\n",
    "<a id=\"part2kaggle\"></a>\n",
    "\n",
    "## CS109B Kaggle Competition\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**ACCESS AND JOIN THE COMPETITION**:\n",
    "\n",
    "**You need to create an account on Kaggle and [join the competition via this link](https://www.kaggle.com/t/01aa5d4183fd4c429e4563808369b2a2). This is a limited participation competition. Please DO NOT share this link.**\n",
    "\n",
    "**For more information on the rules** governing this CS109B Kaggle competition, please see below and also review [the modelling restrictions DOS and DON'TS outlined in question 2.3.1](#q2.3.1).\n",
    "\n",
    "**IMPORTANT NOTES ABOUT SCORING**:\n",
    "\n",
    "- The **public leaderboard** on Kaggle displays your performance on only 30% of the test set.\n",
    "\n",
    "\n",
    "- After the competition is complete, the **private leaderboard** will show your performance on the remaining 70% of the test set.\n",
    "\n",
    "- Question 2.3.5 is worth 5 points (the entire notebook is scored out of 100). Points for this questions will be awarded based on the **private leaderboard** as follows.\n",
    "\n",
    "|Private Leaderboard Score   | Points|\n",
    "|----------------------------|-------|\n",
    "|$0.945 \\leq \\text{score}$ |5|\n",
    "|$0.944 \\leq \\text{score} \\lt 0.945$|4|\n",
    "|$0.93 \\leq \\text{score} \\lt 0.94$|3|\n",
    "|$0.92 \\leq \\text{score} \\lt 0.93$|2|\n",
    "|$0.889 \\leq \\text{score} \\lt 0.92$|1|\n",
    "\n",
    "**üèÜ Grand Prize(s):** The **Top-4** students on the private leaderboard will win an invitation to dinner with Pavlos and some of the CS109B teaching staff. This refers to the top 4 individuals on the board. The invitation will be extended to 5 students if the restriction to 4 would otherwise divide a group (e.g., 3 single submissions at the top followed by a group of 2).\n",
    "\n",
    "**ADDITIONAL COMPETITION RULES:**\n",
    "\n",
    "- Multiple Kaggle submissions are permitted (with a maximum of 20 submissions per-participant per-day), **just note that you will need to choose, on Kaggle, the ONE single submission to use for final scoring prior to the final HW3 submission deadline**, and **your submitted notebook MUST contain the matching code and model that generated your ONE chosen submission.**\n",
    "\n",
    "\n",
    "- **To repeat this point, the version of your final HW3 notebook submitted on Canvas MUST contain the same code and exact same model used to generate your ONE chosen Kaggle submission.** (TFs may rerun your notebook code to ensure comparable final leaderboard results.)\n",
    "\n",
    "\n",
    "- **Please do not manually label your submissions.** In other words, the labels should only be the outcome of your model.\n",
    "\n",
    "\n",
    "- **No external data are allowed, you MUST USE ONLY the KMNIST training and test data downloaded via the \"Data\" tab of [the CS109B competition page linked above](#part2data).**\n",
    "\n",
    "\n",
    "- **Do not** create multiple accounts on Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1**  **Get and visualize the data**\n",
    "\n",
    "- Download the train and test data from [the competition page](#part2data).\n",
    "- We will utilize `kmnist_test.csv` in question 2.3.4 only. \n",
    "- Load the data and use the matplotlib function `imshow` to display a handwritten 0 and a handwritten 1 from the training set.\n",
    "- You are responsible for any preprocessing you deem necessary to help in your prediction task.\n",
    "\n",
    "<a id=\"q22\"></a>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2**  **Overfit an ANN** \n",
    "\n",
    "Build and fit a fully-connected network (FCN) with the architecture given below using `tensorflow.keras` and assign it to a variable called `model_overfit`:\n",
    "\n",
    "- Number of hidden layers: 3\n",
    "- Nodes per hidden layer: 100, 100, 100\n",
    "- Activation function: ReLU \n",
    "- Loss function: binary_crossentropy\n",
    "- Output unit: Sigmoid \n",
    "- Optimizer: adam (use the defaults; no other tuning)\n",
    "- Epochs: 1,000\n",
    "- Batch size: 128\n",
    "- Validation size: 0.3\n",
    "\n",
    "![diagram](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F6a491ff8d4ff590dc8ded9a25461cd4b%2FScreenshot%202020-10-20%20at%209.42.36%20PM.png?generation=1603210420701577&alt=media) \n",
    "    \n",
    "This ANN, when trained on the dataset, will overfit to the training set. Plot the training accuracy and validation accuracy (the x-axis should represent the number of epochs, and the y-axis should represent the accuracy). Explain how you can tell the model is overfitting. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3.1**  **Regularize the overfit network**\n",
    "\n",
    "Create an ANN that doesn't overfit and use it to [compete on Kaggle](#part2kaggle).\n",
    "\n",
    "<a id=\"q2.3.1\"></a>\n",
    "    \n",
    "**DON'TS**\n",
    "\n",
    "- **DO NOT change the architecture**. In other words, keep the same number of layers, same number of nodes, same activation function, and same loss function and output unit as was used in your question 2.2 overfit model. **No CNNs, RNNs, ensembles, or fancy enhancements.**\n",
    "\n",
    "- **DO NOT manually label your submissions.** In other words, the labels should only be the outcome of your model.\n",
    "\n",
    "- **DO NOT use any external data.** Please use ONLY the specific KMNIST datasets provided to you (via the CS109B [data link above](#part2data)) for training your model and for generating your test predictions.\n",
    "\n",
    "**DOS**\n",
    "\n",
    " - **YOU CAN change the** number of epochs (max 2000), batch size, optimizer, and of course, add elements that can help to regularize your model (e.g., dropout, L2 norm, etc.).\n",
    " - **YOU CAN also** do data augmentation using the provided training data. \n",
    " - **YOU CAN** add flatten layers as you see fit.\n",
    "\n",
    "**IMPORTANT: YOU MUST** ensure that the version of the code and model in your final submitted notebook is the **EXACT SAME** code and model used to generate your Kaggle submission. TFs may run your submitted model to ensure comparable results. **Other Kaggle competition rules and scoring details [are listed here](#part2kaggle).**\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3.2**  Plot your model's training accuracy and validation accuracy as a function of epochs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3.3**  In a few sentences, describe the various approaches you have taken to improve the performance of your regularized model in 2.3.1 as well as any observations you might have regarding your training and Kaggle results.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3.4**  Generate your test-set class predictions using your regularized model. Save those predictions to a `.csv` formatted file. Submit that `.csv` file [to the CS109B Kaggle Competition](#part2kaggle) for leaderboard scoring. **IMPORTANT:** For Kaggle to accept and score your submitted `.csv` file, it MUST contain 2 columns. The first column must be named `\"Id\"` and needs to contain the test observation index numbers corresponding to each of your 2,000 predictions (index starting at `0`), the second column must be named `\"Category\"` and needs to contain your class predictions (i.e. `0` or `1`) for each corresponding test observation index location. Both columns should contain integer data types.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3.5**  **Specify your Kaggle name that you have used on the leaderboard**. We CANNOT give you credit without this.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'><b>2.4 Wrap-up</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "* In a few sentences, please describe the aspect(s) of the assignment you found most challenging. This could be conceptual and/or related to coding and implementation.\n",
    "\n",
    "* How many hours did you spend working on this assignment? Store this as an int or float in `hours_spent_on_hw`. If you worked on the project in a group, report the *average* time spent per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hours_spent_on_hw = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "print(f\"It took {(time_end - time_start)/60:.2f} minutes for this notebook to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This concludes HW3. Thank you!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q2.4": {
     "name": "q2.4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
